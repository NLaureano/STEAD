import matplotlib.pyplot as plt
import numpy as np

Training_Losses = [[780.3878665268421, 581.6063185892999, 509.24479307979345, 473.68092453479767, 435.40785744041204, 409.9606028422713, 386.8613458201289, 383.1757463812828, 361.79274827986956, 346.7525302246213], [776.4225708395243, 539.6479548513889, 503.95510017871857, 481.2114467471838, 465.536960683763, 435.6762602701783, 407.45245469361544, 387.27348294854164, 366.6152773499489, 348.679227553308], [841.2095796465874, 568.2857456505299, 483.4862739741802, 457.5858663097024, 453.79579884558916, 413.00536911934614, 392.6221082061529, 409.8684525862336, 380.01608975976706, 367.42171075195074], [990.6524220705032, 622.4001437574625, 538.1126797050238, 489.4057023078203, 486.61567682772875, 460.04761277139187, 428.297591149807, 417.99451257288456, 417.3618930131197, 418.8829487338662]]
Training_Accuracies = [[0.7289428571428571, 0.7939142857142857, 0.8184, 0.8315142857142858, 0.8422285714285714, 0.8528571428571429, 0.8578857142857143, 0.862, 0.8676857142857143, 0.8721428571428571], [0.7370571428571429, 0.8049142857142857, 0.8162285714285714, 0.8234, 0.8263428571428572, 0.838, 0.8469142857142857, 0.8552571428571428, 0.8594571428571428, 0.8644], [0.7157428571428571, 0.7896857142857143, 0.8162571428571429, 0.8236857142857142, 0.8285714285714286, 0.8378285714285715, 0.8428, 0.8438285714285715, 0.8508, 0.8574285714285714], [0.6723428571428571, 0.7699428571428572, 0.8027714285714286, 0.8177142857142857, 0.8169428571428572, 0.8274, 0.834, 0.8385714285714285, 0.8427142857142857, 0.8428]]
Validation_Losses = [[0.6271174035634205, 0.5896686325027685, 0.523745452617384, 0.5571746066877037, 0.5230749579751568, 0.6285985271642163, 0.6500235662149016, 0.5262037481471991, 0.49357809591445195, 0.5077917370352016], [1.872074731216309, 0.7414536414442549, 0.7337636250029703, 0.7094546020220799, 1.0167939792013472, 0.5955478801469135, 0.6761993825625462, 0.5525030018702434, 0.5761520881561717, 0.5346236705400382], [0.7448047016076981, 1.0641262547890091, 0.6455805394679878, 0.6867536638572718, 0.6816401791990183, 0.707659982571936, 0.7040170328632281, 0.5721463266831295, 0.8069588844278816, 0.6675466004830257], [0.8653136119721042, 1.0585165038989608, 0.7571059402766501, 0.7568725573409135, 0.7346201937669402, 0.7861211067362196, 0.5777224520589136, 1.1513410559885062, 0.585433115435254, 0.633964406456917]]
Validation_Accuracies = [[0.7362, 0.763, 0.8076, 0.7988, 0.7818, 0.7914, 0.7744, 0.8102, 0.8174, 0.8316], [0.543, 0.7966, 0.7748, 0.7838, 0.7204, 0.8058, 0.7974, 0.8142, 0.772, 0.788], [0.7374, 0.781, 0.7622, 0.7312, 0.749, 0.7324, 0.7094, 0.7832, 0.7846, 0.7396], [0.7308, 0.6718, 0.7656, 0.7778, 0.7432, 0.7564, 0.7672, 0.3064, 0.783, 0.7494]]

learning_rates = ['1e-4', '5e-4', '1e-3', '3e-3']
epochs = list(range(1, 11))

# Plot Training Loss
plt.figure(figsize=(10, 6))
for i, loss in enumerate(Training_Losses):
    plt.plot(epochs, loss, label=f'lr={learning_rates[i]}')
plt.title("Training Loss vs Epochs")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Plot Training Accuracy
plt.figure(figsize=(10, 6))
for i, acc in enumerate(Training_Accuracies):
    plt.plot(epochs, acc, label=f'lr={learning_rates[i]}')
plt.title("Training Accuracy vs Epochs")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Plot Validation Loss
plt.figure(figsize=(10, 6))
for i, val_loss in enumerate(Validation_Losses):
    plt.plot(epochs, val_loss, label=f'lr={learning_rates[i]}')
plt.title("Validation Loss vs Epochs")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Plot Validation Accuracy
plt.figure(figsize=(10, 6))
for i, val_acc in enumerate(Validation_Accuracies):
    plt.plot(epochs, val_acc, label=f'lr={learning_rates[i]}')
plt.title("Validation Accuracy vs Epochs")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()